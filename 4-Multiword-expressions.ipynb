{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiword expressions identification and extraction\n",
    "- Use SpaCy tokenizer API to tokenize the text from the FIQA corpus (from the 1 lab).\n",
    "-Compute bigram counts of downcased tokens. \n",
    "- Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts.\n",
    "- Use pointwise mutual information to compute the measure for all pairs of words.\n",
    "- Sort the word pairs according to that measure in the descending order and determine top 10 entries.\n",
    "- Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences).\n",
    "- Use SpaCy to lemmatize and tag the sentences in the corpus.\n",
    "- Using the tagged corpus compute bigram statistic for the tokens containing: a. lemmatized, downcased word b. morphosyntactic category of the word (subst, fin, adj, etc.)\n",
    "\n",
    "- Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences.\n",
    "- Group the bigrams by morphosyntactic tag, i.e. a pair of words belongs to a given group if all pairs have the same syntactic category for the first and the second word. E.g. one group would be words with subst as the first words and adj as the second word.\n",
    "- Print top-10 categories (sort them by total count of bigrams) and print top-5 pairs for each category.\n",
    "- Create a table comparing the results for copora without and with tagging and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milenabiernacka/anaconda3/envs/PJN/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SpaCy tokenizer API to tokenize the text from the FIQA corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading spacy model\n",
    "nlp = spacy.load(\"pl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "dataset = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "df = pd.DataFrame(dataset['corpus'])\n",
    "df_text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy tokenization\n",
    "# tokenized_texts = []\n",
    "\n",
    "# for text in df_text:\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.text for token in doc]\n",
    "#     tokenized_texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenized corpus saved in previous report\n",
    "corpus_tokenized = pd.read_csv(\"tokenized_texts.csv\", sep = ';', converters={'tokens': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one list of tokens from many lists\n",
    "for row in corpus_tokenized['tokens']:\n",
    "    for token in row:\n",
    "        corpus_tokens.append(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nie', 'mówię', ',', 'że', 'nie', 'podoba', 'mi', 'się', 'też', 'pomysł']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigram counts of downcased tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nie', 'mówię'),\n",
       " ('mówię', ','),\n",
       " (',', 'że'),\n",
       " ('że', 'nie'),\n",
       " ('nie', 'podoba'),\n",
       " ('podoba', 'mi'),\n",
       " ('mi', 'się'),\n",
       " ('się', 'też'),\n",
       " ('też', 'pomysł'),\n",
       " ('pomysł', 'szkolenia')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing bigrams\n",
    "bi_grams = list(bigrams(corpus_tokens))\n",
    "bi_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating bigrams frequency\n",
    "bigram_freq = FreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nie mówię\": 275\n",
      "\"że nie\": 5013\n",
      "\"nie podoba\": 164\n",
      "\"podoba mi\": 234\n",
      "\"mi się\": 1398\n",
      "\"się też\": 98\n"
     ]
    }
   ],
   "source": [
    "# see results\n",
    "for idx, (bigram, count) in enumerate(bigram_freq.items()):\n",
    "    print(f'\"{bigram[0]} {bigram[1]}\": {count}')\n",
    "    if idx == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard bigrams containing characters other than letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore bigrams containing characters other than letters\n",
    "bigram_freq = {bigram: count for bigram, count in bigram_freq.items() if all(word.isalpha() for word in bigram)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nie mówię\": 275\n",
      "\"że nie\": 5013\n",
      "\"nie podoba\": 164\n",
      "\"podoba mi\": 234\n",
      "\"mi się\": 1398\n",
      "\"się też\": 98\n"
     ]
    }
   ],
   "source": [
    "# see results\n",
    "for idx, (bigram, count) in enumerate(bigram_freq.items()):\n",
    "    print(f'\"{bigram[0]} {bigram[1]}\": {count}')\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Use pointwise mutual information to compute the measure for all pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unigram freequency for PMI \n",
    "unigram_freq = FreqDist(corpus_tokens)\n",
    "unigram_freq = {unigram: count for unigram, count in unigram_freq.items() if all(word.isalpha() for word in unigram)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nie\": 131454\n",
      "\"mówię\": 978\n",
      "\"że\": 90019\n",
      "\"podoba\": 503\n",
      "\"mi\": 5497\n",
      "\"się\": 85840\n"
     ]
    }
   ],
   "source": [
    "# see results\n",
    "for idx, (unigram, count) in enumerate(unigram_freq.items()):\n",
    "    print(f'\"{unigram}\": {count}')\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculating PMI for bigrams\n",
    "def calculate_pmi_for_corpus(bigrams, unigram_freq, bigram_freq):\n",
    "    total_bigrams = sum(bigram_freq.values())\n",
    "    pmi_scores = {}\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram\n",
    "        p_a = unigram_freq.get(word_a, 0) / total_bigrams\n",
    "        p_b = unigram_freq.get(word_b, 0) / total_bigrams\n",
    "        p_a_b = bigram_freq.get(bigram, 0) / total_bigrams\n",
    "\n",
    "        if p_a * p_b != 0 and p_a_b != 0:\n",
    "            pmi = math.log2(p_a_b / (p_a * p_b))\n",
    "            pmi_scores[bigram] = pmi\n",
    "\n",
    "    return pmi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pmi on our corpus bigrams\n",
    "pmi_scores = calculate_pmi_for_corpus(bi_grams, unigram_freq, bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nie mówię\": 3.4418706758592106\n",
      "\"że nie\": 1.1057901233813492\n",
      "\"nie podoba\": 3.655410937211979\n",
      "\"podoba mi\": 8.74799340481655\n",
      "\"mi się\": 3.911829464770224\n",
      "\"się też\": 0.5396463964380495\n",
      "\"też pomysł\": 1.4408548954513487\n",
      "\"pomysł szkolenia\": 4.730602584589561\n",
      "\"szkolenia w\": 0.5798881871130837\n",
      "\"w miejscu\": 2.3930893671063753\n"
     ]
    }
   ],
   "source": [
    "# see results\n",
    "for idx, (bigram, pmi) in enumerate(pmi_scores.items()):\n",
    "    print(f'\"{bigram[0]} {bigram[1]}\": {pmi}')\n",
    "    if idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the word pairs according to that measure in the descending order and determine top 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting pmi scores based on pmi value\n",
    "sorted_pmi_scores = sorted(pmi_scores.items(), key=lambda x: x[1], reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('jankesem', 'skrzeczącym'), 22.276472039076435)\n",
      "(('wizach', 'panamskich'), 22.276472039076435)\n",
      "(('devrait', 'éviter'), 22.276472039076435)\n",
      "(('facturer', 'fortement'), 22.276472039076435)\n",
      "(('peletki', 'chmielowe'), 22.276472039076435)\n",
      "(('opalaniem', 'wzmiankowane'), 22.276472039076435)\n",
      "(('wielowęzłowy', 'sekwencer'), 22.276472039076435)\n",
      "(('sonarowe', 'zamontowane'), 22.276472039076435)\n",
      "(('napompowanymi', 'gpas'), 22.276472039076435)\n",
      "(('непрекъсната', 'памет'), 22.276472039076435)\n"
     ]
    }
   ],
   "source": [
    "# see top 10 pmi scores\n",
    "for idx, (bigram, pmi) in enumerate(sorted_pmi_scores):\n",
    "    print((bigram, pmi))\n",
    "    if idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received PMI scores seem very high, suggesting that these bigrams occur very rarely, but when they do occur, they are very characteristic of the text in question. We can see that in our corpus some bigrams that have the highest pmi scores are phrases not in polish language. On the other hand polish bigrams with high PMI are very specific combinations of 2 words that are not commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering only bigrams that occure 5 times\n",
    "bigrams_freq_more_than_5 = {bigram: count for bigram, count in bigram_freq.items() if count >= 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pmi on our corpus bigrams that occur more than 5 times\n",
    "pmi_scores_more_than_5 = calculate_pmi_for_corpus(bi_grams, unigram_freq, bigrams_freq_more_than_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting pmi scores based on pmi value\n",
    "sorted_pmi_scores_more_than_5 = sorted(pmi_scores_more_than_5.items(), key=lambda x: x[1], reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('klęska', 'żywiołowa'), 19.279373896686394)\n",
      "(('bert', 'hellinger'), 19.279373896686394)\n",
      "(('królicza', 'nora'), 19.279373896686394)\n",
      "(('инарные', 'опционы'), 19.279373896686394)\n",
      "(('опционы', 'олимп'), 19.279373896686394)\n",
      "(('олимп', 'трейд'), 19.279373896686394)\n",
      "(('мою', 'команду'), 19.279373896686394)\n",
      "(('моя', 'группа'), 19.279373896686394)\n",
      "(('stucco', 'veneziano'), 19.279373896686394)\n",
      "(('остались', 'вопросы'), 19.279373896686394)\n"
     ]
    }
   ],
   "source": [
    "# see top 10 pmi scores\n",
    "for idx, (bigram, pmi) in enumerate(sorted_pmi_scores_more_than_5):\n",
    "    print((bigram, pmi))\n",
    "    if idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining top 10 PMI scores are still very high and again suggest that bigrams are relevant phenomena in the context of the text. Now again we can see that most of these 10 bigrams are non polish words (maily russian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SpaCy to lemmatize and tag the sentences in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize and tag sentences\n",
    "def lemmatize_and_tag(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_and_tagged = [(token.lemma_.lower(), token.tag_) for token in doc]\n",
    "    return lemmatized_and_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ala', 'SUBST'), ('mieć', 'FIN'), ('kot', 'SUBST')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_and_tag(\"Ala ma kota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lemmatized_and_tagged'] = df['text'].apply(lemmatize_and_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"df_lemmatized_tagged\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_lemmatized_tagged.csv\", converters={'lemmatized_and_tagged': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lemmatized_and_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nie mówię, że nie podoba mi się też pomysł szk...</td>\n",
       "      <td>[(nie, QUB), (mówić, FIN), (,, INTERP), (że, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tak więc nic nie zapobiega fałszywym ocenom po...</td>\n",
       "      <td>[(tak, ADV), (więc, CONJ), (nic, SUBST), (nie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nigdy nie możesz korzystać z FSA dla indywidua...</td>\n",
       "      <td>[(nigdy, ADV), (nie, QUB), (móc, FIN), (korzys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Samsung stworzył LCD i inne technologie płaski...</td>\n",
       "      <td>[(samsung, SUBST), (stworzyć, PRAET), (lcd, SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oto wymagania SEC: Federalne przepisy dotycząc...</td>\n",
       "      <td>[(oto, QUB), (wymaganie, SUBST), (sec, SUBST),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id  title                                               text  \\\n",
       "0    3    NaN  Nie mówię, że nie podoba mi się też pomysł szk...   \n",
       "1   31    NaN  Tak więc nic nie zapobiega fałszywym ocenom po...   \n",
       "2   56    NaN  Nigdy nie możesz korzystać z FSA dla indywidua...   \n",
       "3   59    NaN  Samsung stworzył LCD i inne technologie płaski...   \n",
       "4   63    NaN  Oto wymagania SEC: Federalne przepisy dotycząc...   \n",
       "\n",
       "                               lemmatized_and_tagged  \n",
       "0  [(nie, QUB), (mówić, FIN), (,, INTERP), (że, C...  \n",
       "1  [(tak, ADV), (więc, CONJ), (nic, SUBST), (nie,...  \n",
       "2  [(nigdy, ADV), (nie, QUB), (móc, FIN), (korzys...  \n",
       "3  [(samsung, SUBST), (stworzyć, PRAET), (lcd, SU...  \n",
       "4  [(oto, QUB), (wymaganie, SUBST), (sec, SUBST),...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tagged corpus compute bigram statistic for the tokens containing: \n",
    "\n",
    "a. lemmatized, downcased word \n",
    "\n",
    "b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute bigram statistics based on lemmatized, lowercase words\n",
    "def compute_lemmatized_bigram_statistics(lemmatized_paragraphs):\n",
    "    lemmatized_bigram_statistics = Counter()\n",
    "\n",
    "    for lemmatized_paragraph in lemmatized_paragraphs:\n",
    "        # Extract lemmatized bigrams\n",
    "        for i in range(len(lemmatized_paragraph) - 1):\n",
    "            lemmatized_word_a, _ = lemmatized_paragraph[i]\n",
    "            lemmatized_word_b, _ = lemmatized_paragraph[i + 1]\n",
    "            \n",
    "            lemmatized_bigram = (lemmatized_word_a, lemmatized_word_b)\n",
    "            lemmatized_bigram_statistics[lemmatized_bigram] += 1\n",
    "\n",
    "    return lemmatized_bigram_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute bigram statistics based on morphosyntactic categories\n",
    "def compute_category_bigram_statistics(tagged_paragraphs):\n",
    "    category_bigram_statistics = Counter()\n",
    "\n",
    "    for tagged_paragraph in tagged_paragraphs:\n",
    "        # Extract category bigrams\n",
    "        for i in range(len(tagged_paragraph) - 1):\n",
    "            _, category_a = tagged_paragraph[i]\n",
    "            _, category_b = tagged_paragraph[i + 1]\n",
    "            \n",
    "            category_bigram = (category_a, category_b)\n",
    "            category_bigram_statistics[category_bigram] += 1\n",
    "\n",
    "    return category_bigram_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with DataFrame\n",
    "lemmatized_bigram_stats = compute_lemmatized_bigram_statistics(df['lemmatized_and_tagged'])\n",
    "category_bigram_stats = compute_category_bigram_statistics(df['lemmatized_and_tagged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Lemmatized Bigrams:\n",
      ", że - Count: 86095\n",
      ", który - Count: 55544\n",
      ", a - Count: 32643\n",
      ", ale - Count: 32549\n",
      "to , - Count: 28987\n",
      ", aby - Count: 28290\n",
      "nie być - Count: 25266\n",
      ", co - Count: 20579\n",
      "być to - Count: 19794\n",
      ". jeśli - Count: 19470\n",
      "\n",
      "Top 10 Category Bigrams:\n",
      "SUBST SUBST - Count: 913780\n",
      "SUBST INTERP - Count: 646717\n",
      "ADJ SUBST - Count: 496638\n",
      "PREP SUBST - Count: 423126\n",
      "SUBST PREP - Count: 284892\n",
      "SUBST ADJ - Count: 242427\n",
      "SUBST FIN - Count: 177841\n",
      "INTERP COMP - Count: 176484\n",
      "PREP ADJ - Count: 171353\n",
      "INTERP SUBST - Count: 157059\n"
     ]
    }
   ],
   "source": [
    "# see top 10 lemmatized bigrams\n",
    "print(\"Top 10 Lemmatized Bigrams:\")\n",
    "for lemmatized_bigram, count in lemmatized_bigram_stats.most_common(10):\n",
    "    print(f\"{lemmatized_bigram[0]} {lemmatized_bigram[1]} - Count: {count}\")\n",
    "\n",
    "# see top 10 category bigrams\n",
    "print(\"\\nTop 10 Category Bigrams:\")\n",
    "for category_bigram, count in category_bigram_stats.most_common(10):\n",
    "    print(f\"{category_bigram[0]} {category_bigram[1]} - Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below the same  but not considering punctuation, just out of curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Lemmatized Bigrams (Without Punctuation):\n",
      "nie być - Count: 25427\n",
      "być to - Count: 19924\n",
      "nie mieć - Count: 14496\n",
      "to że - Count: 11124\n",
      "w ten - Count: 10288\n",
      "po prosty - Count: 9764\n",
      "być w - Count: 8959\n",
      "to być - Count: 7715\n",
      "móc być - Count: 7458\n",
      "to co - Count: 7191\n",
      "\n",
      "Top 10 Category Bigrams (Without Punctuation):\n",
      "SUBST SUBST - Count: 592574\n",
      "ADJ SUBST - Count: 498747\n",
      "PREP SUBST - Count: 422059\n",
      "SUBST PREP - Count: 321584\n",
      "SUBST ADJ - Count: 307722\n",
      "SUBST FIN - Count: 225846\n",
      "SUBST CONJ - Count: 195645\n",
      "PREP ADJ - Count: 171523\n",
      "FIN SUBST - Count: 141214\n",
      "SUBST COMP - Count: 122852\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punctuation(lemmatized_paragraph):\n",
    "    return [(word, tag) for word, tag in lemmatized_paragraph if not any(char in string.punctuation for char in word)]\n",
    "\n",
    "# Function to compute bigram statistics based on lemmatized, lowercase words, no punctuation\n",
    "def compute_lemmatized_bigram_statistics_no_punct(lemmatized_paragraphs):\n",
    "    lemmatized_bigram_statistics = Counter()\n",
    "\n",
    "    for lemmatized_paragraph in lemmatized_paragraphs:\n",
    "        lemmatized_paragraph_no_punct = remove_punctuation(lemmatized_paragraph)\n",
    "\n",
    "        for i in range(len(lemmatized_paragraph_no_punct) - 1):\n",
    "            lemmatized_word_a, _ = lemmatized_paragraph_no_punct[i]\n",
    "            lemmatized_word_b, _ = lemmatized_paragraph_no_punct[i + 1]\n",
    "\n",
    "            lemmatized_bigram = (lemmatized_word_a, lemmatized_word_b)\n",
    "            lemmatized_bigram_statistics[lemmatized_bigram] += 1\n",
    "\n",
    "    return lemmatized_bigram_statistics\n",
    "\n",
    "# Function to compute bigram statistics based on morphosyntactic categories, no punctuation \n",
    "def compute_category_bigram_statistics_no_punct(tagged_paragraphs):\n",
    "    category_bigram_statistics = Counter()\n",
    "\n",
    "    for tagged_paragraph in tagged_paragraphs:\n",
    "        tagged_paragraph_no_punct = remove_punctuation(tagged_paragraph)\n",
    "\n",
    "        for i in range(len(tagged_paragraph_no_punct) - 1):\n",
    "            _, category_a = tagged_paragraph_no_punct[i]\n",
    "            _, category_b = tagged_paragraph_no_punct[i + 1]\n",
    "\n",
    "            category_bigram = (category_a, category_b)\n",
    "            category_bigram_statistics[category_bigram] += 1\n",
    "\n",
    "    return category_bigram_statistics\n",
    "\n",
    "lemmatized_bigram_stats_no_punct = compute_lemmatized_bigram_statistics_no_punct(df['lemmatized_and_tagged'])\n",
    "category_bigram_stats_no_punct = compute_category_bigram_statistics_no_punct(df['lemmatized_and_tagged'])\n",
    "\n",
    "print(\"Top 10 Lemmatized Bigrams (Without Punctuation):\")\n",
    "for lemmatized_bigram, count in lemmatized_bigram_stats_no_punct.most_common(10):\n",
    "    print(f\"{lemmatized_bigram[0]} {lemmatized_bigram[1]} - Count: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Category Bigrams (Without Punctuation):\")\n",
    "for category_bigram, count in category_bigram_stats_no_punct.most_common(10):\n",
    "    print(f\"{category_bigram[0]} {category_bigram[1]} - Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts = df['lemmatized_and_tagged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_bigrams = []\n",
    "for lemmatized_paragraph in lemmatized_texts:\n",
    "    lemmatized_bigrams = [(word_a, word_b) for (word_a, _), (word_b, _) in zip(lemmatized_paragraph, lemmatized_paragraph[1:])]\n",
    "    lem_bigrams.append(lemmatized_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_bigrams = [bigram for bigram_list in lem_bigrams for bigram in bigram_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nie', 'mówić'),\n",
       " ('mówić', ','),\n",
       " (',', 'że'),\n",
       " ('że', 'nie'),\n",
       " ('nie', 'podobać')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utwórz unigram_freq\n",
    "lem_unigram_freq = Counter()\n",
    "for lemmatized_paragraph in lemmatized_texts:\n",
    "    for word, _ in lemmatized_paragraph:\n",
    "        lem_unigram_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nie', 130916)\n",
      "('mówić', 7895)\n",
      "(',', 611388)\n",
      "('że', 90022)\n",
      "('podobać', 667)\n",
      "('ja', 12144)\n"
     ]
    }
   ],
   "source": [
    "for idx, (unigram, count) in enumerate(lem_unigram_freq.items()):\n",
    "    print((unigram, count))\n",
    "    if idx ==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigram_freq\n",
    "lem_bigram_freq = Counter()\n",
    "for lemmatized_paragraph in lemmatized_texts:\n",
    "    lemmatized_bigrams = [(word_a, word_b) for (word_a, _), (word_b, _) in zip(lemmatized_paragraph, lemmatized_paragraph[1:])]\n",
    "    lem_bigram_freq.update(lemmatized_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('nie', 'mówić'), 825)\n",
      "(('mówić', ','), 3316)\n",
      "((',', 'że'), 86095)\n",
      "(('że', 'nie'), 5014)\n",
      "(('nie', 'podobać'), 214)\n",
      "(('podobać', 'ja'), 301)\n"
     ]
    }
   ],
   "source": [
    "for idx, (bigram, count) in enumerate(lem_bigram_freq.items()):\n",
    "    print((bigram, count))\n",
    "    if idx ==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore bigrams containing characters other than letters\n",
    "lem_bigram_freq = {bigram: count for bigram, count in lem_bigram_freq.items() if all(word.isalpha() for word in bigram)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering only bigrams that occure 5 times\n",
    "lem_bigrams_freq_more_than_5 = {bigram: count for bigram, count in lem_bigram_freq.items() if count >= 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_pmi_scores = calculate_pmi_for_corpus(lem_bigrams, lem_unigram_freq, lem_bigrams_freq_more_than_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting pmi scores based on pmi value\n",
    "sorted_lem_pmi_scores = sorted(lem_pmi_scores.items(), key=lambda x: x[1], reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('emiratów', 'arabskich'), 19.476754113430072)\n",
      "(('bert', 'hellinger'), 19.476754113430072)\n",
      "(('инарные', 'опционы'), 19.476754113430072)\n",
      "(('опционы', 'олимп'), 19.476754113430072)\n",
      "(('олимп', 'трейд'), 19.476754113430072)\n",
      "(('мою', 'команду'), 19.476754113430072)\n",
      "(('моя', 'группа'), 19.476754113430072)\n",
      "(('stucco', 'veneziano'), 19.476754113430072)\n",
      "(('остались', 'вопросы'), 19.476754113430072)\n",
      "(('экономическая', 'игра'), 19.213719707596276)\n"
     ]
    }
   ],
   "source": [
    "# see top 10 pmi scores\n",
    "for idx, (bigram, pmi) in enumerate(sorted_lem_pmi_scores):\n",
    "    print((bigram, pmi))\n",
    "    if idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the bigrams by morphosyntactic tag, i.e. a pair of words belongs to a given group if all pairs have the same syntactic category for the first and the second word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all bigrams with tags\n",
    "all_bigrams_with_tags = []\n",
    "\n",
    "for lemmatized_paragraph in lemmatized_texts:\n",
    "    lemmatized_bigrams_with_tags = [((word_a, tag_a), (word_b, tag_b)) for (word_a, tag_a), (word_b, tag_b) in zip(lemmatized_paragraph, lemmatized_paragraph[1:])]\n",
    "    all_bigrams_with_tags.extend(lemmatized_bigrams_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('nie', 'QUB'), ('mówić', 'FIN')),\n",
       " (('mówić', 'FIN'), (',', 'INTERP')),\n",
       " ((',', 'INTERP'), ('że', 'COMP')),\n",
       " (('że', 'COMP'), ('nie', 'QUB')),\n",
       " (('nie', 'QUB'), ('podobać', 'FIN')),\n",
       " (('podobać', 'FIN'), ('ja', 'PPRON12')),\n",
       " (('ja', 'PPRON12'), ('się', 'QUB')),\n",
       " (('się', 'QUB'), ('też', 'QUB')),\n",
       " (('też', 'QUB'), ('pomysł', 'SUBST')),\n",
       " (('pomysł', 'SUBST'), ('szkolenie', 'GER'))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bigrams_with_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Słownik do przechowywania grupowanych bigramów\n",
    "grouped_bigrams = defaultdict(list)\n",
    "\n",
    "# Grupowanie bigramów\n",
    "for bigram in all_bigrams_with_tags:\n",
    "    first_word_tag = bigram[0][1]\n",
    "    second_word_tag = bigram[1][1]\n",
    "    \n",
    "    # Klucz grupy to krotka z tagami obu słów\n",
    "    group_key = (first_word_tag, second_word_tag)\n",
    "    \n",
    "    # Dodaj bigram do odpowiedniej grupy\n",
    "    grouped_bigrams[group_key].append(bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liczenie łącznej liczby bigramów dla każdej grupy\n",
    "group_counts = Counter()\n",
    "for group_key, group in grouped_bigrams.items():\n",
    "    group_counts[group_key] = len(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category ('SUBST', 'SUBST'), Total Count: 913780\n",
      "(('miejsce', 'SUBST'), ('praca', 'SUBST'))\n",
      "(('student', 'SUBST'), (')', 'SUBST'))\n",
      "(('nic', 'SUBST'), ('wykwalifikować', 'SUBST'))\n",
      "(('wykwalifikować', 'SUBST'), ('.', 'SUBST'))\n",
      "(('strona', 'SUBST'), ('rynek', 'SUBST'))\n",
      "\n",
      "Category ('SUBST', 'INTERP'), Total Count: 646717\n",
      "(('praca', 'SUBST'), (',', 'INTERP'))\n",
      "(('zrobić', 'SUBST'), ('.', 'INTERP'))\n",
      "(('praca', 'SUBST'), ('–', 'INTERP'))\n",
      "(('oprogramowanie', 'SUBST'), ('.', 'INTERP'))\n",
      "(('edukacja', 'SUBST'), (',', 'INTERP'))\n",
      "\n",
      "Category ('ADJ', 'SUBST'), Total Count: 496638\n",
      "(('ogromny', 'ADJ'), ('inwestycja', 'SUBST'))\n",
      "(('fałszywy', 'ADJ'), ('ocen', 'SUBST'))\n",
      "(('dodatkowy', 'ADJ'), ('kontrola', 'SUBST'))\n",
      "(('nowoki', 'ADJ'), ('kontrola', 'SUBST'))\n",
      "(('należyty', 'ADJ'), ('staranność', 'SUBST'))\n",
      "\n",
      "Category ('PREP', 'SUBST'), Total Count: 423126\n",
      "(('w', 'PREP'), ('miejsce', 'SUBST'))\n",
      "(('w', 'PREP'), ('stany', 'SUBST'))\n",
      "(('w', 'PREP'), ('edukacja', 'SUBST'))\n",
      "(('z', 'PREP'), ('tysiąc', 'SUBST'))\n",
      "(('do', 'PREP'), ('nic', 'SUBST'))\n",
      "\n",
      "Category ('SUBST', 'PREP'), Total Count: 284892\n",
      "(('inwestycja', 'SUBST'), ('w', 'PREP'))\n",
      "(('ocen', 'SUBST'), ('poza', 'PREP'))\n",
      "(('kontrola', 'SUBST'), ('z', 'PREP'))\n",
      "(('.', 'SUBST'), ('w', 'PREP'))\n",
      "(('staranność', 'SUBST'), ('przy', 'PREP'))\n",
      "\n",
      "Category ('SUBST', 'ADJ'), Total Count: 242427\n",
      "(('system', 'SUBST'), ('edukacyjny', 'ADJ'))\n",
      "(('stany', 'SUBST'), ('zjednoczone', 'ADJ'))\n",
      "(('umiejętność', 'SUBST'), ('rynkowy', 'ADJ'))\n",
      "(('ratinga', 'SUBST'), ('kredytowy', 'ADJ'))\n",
      "(('instrument', 'SUBST'), ('finansowy', 'ADJ'))\n",
      "\n",
      "Category ('SUBST', 'FIN'), Total Count: 177841\n",
      "(('intencją', 'SUBST'), ('być', 'FIN'))\n",
      "(('cdo', 'SUBST'), ('być', 'FIN'))\n",
      "(('fsa', 'SUBST'), ('móc', 'FIN'))\n",
      "(('co', 'SUBST'), ('chcieć', 'FIN'))\n",
      "(('składka', 'SUBST'), ('być', 'FIN'))\n",
      "\n",
      "Category ('INTERP', 'COMP'), Total Count: 176484\n",
      "((',', 'INTERP'), ('że', 'COMP'))\n",
      "((',', 'INTERP'), ('że', 'COMP'))\n",
      "((',', 'INTERP'), ('że', 'COMP'))\n",
      "((',', 'INTERP'), ('że', 'COMP'))\n",
      "((',', 'INTERP'), ('że', 'COMP'))\n",
      "\n",
      "Category ('PREP', 'ADJ'), Total Count: 171353\n",
      "(('poza', 'PREP'), ('dodatkowy', 'ADJ'))\n",
      "(('dla', 'PREP'), ('indywidualny', 'ADJ'))\n",
      "(('temu', 'PREP'), ('każdy', 'ADJ'))\n",
      "(('na', 'PREP'), ('który', 'ADJ'))\n",
      "(('dla', 'PREP'), ('mały', 'ADJ'))\n",
      "\n",
      "Category ('INTERP', 'SUBST'), Total Count: 157059\n",
      "(('.', 'INTERP'), ('intencją', 'SUBST'))\n",
      "((',', 'INTERP'), ('co', 'SUBST'))\n",
      "(('.', 'INTERP'), ('samsung', 'SUBST'))\n",
      "((',', 'INTERP'), ('firma', 'SUBST'))\n",
      "((',', 'INTERP'), ('firma', 'SUBST'))\n"
     ]
    }
   ],
   "source": [
    "# Wydrukowanie top-10 kategorii\n",
    "top_categories = group_counts.most_common(10)\n",
    "for category, count in top_categories:\n",
    "    print(f\"\\nCategory {category}, Total Count: {count}\")\n",
    "    \n",
    "    # Wydrukowanie top-5 par dla danej kategorii\n",
    "    top_pairs = grouped_bigrams[category][:5]\n",
    "    for pair in top_pairs:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some thoughts based on obtained results:\n",
    "\n",
    "- ('ADJ', 'SUBST'): This category shows frequent juxtapositions of adjectives with nouns, which can indicate descriptions or characterizations of specific things or situations.\n",
    "- ('PREP', 'SUBST') and ('SUBST', 'PREP'): Combinations of prepositions with nouns suggest some spatial or logical relationship between objects.\n",
    "- ('SUBST', 'SUBST'): In this category, we observe frequent combinations of nouns, suggesting that there are specific phrases or terms in the analyzed text.\n",
    "- ('SUBST', 'FIN'): Juxtapositions of nouns with action verbs indicate descriptions of actions or processes. \n",
    "\n",
    "(nothing very revealing to be honest)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16323c91b843e809beaa456fc31b90062c2b6877cc09dc9168e9867538bda6d2"
  },
  "kernelspec": {
   "display_name": "Python 3.11.6 ('PJN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
